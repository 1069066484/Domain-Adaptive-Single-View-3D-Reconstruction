{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "from models import Encoder, Tnet, _F_, Destructor_domain, Destructor_shape\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, image_root, point_cloud_root, transform=None, use_2048 = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: image directory.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.image_root = image_root\n",
    "        self.point_cloud_root = point_cloud_root\n",
    "        self.transform = transform\n",
    "        self.use_2048 = use_2048\n",
    "        # Init ids\n",
    "        self.ids = []\n",
    "        # Store ids\n",
    "        image_types = os.listdir(image_root)\n",
    "        file_names = []\n",
    "        \n",
    "        for image_type in image_types:\n",
    "            if image_type.endswith('.tgz'):\n",
    "                    continue\n",
    "            \n",
    "            specific_types = os.listdir(os.path.join(image_root, image_type))\n",
    "            \n",
    "            for specific_type in specific_types:\n",
    "                if specific_type.endswith('.tgz'):\n",
    "                    continue\n",
    "                \n",
    "                path1 = os.path.join(os.path.join(os.path.join(image_root, image_type), specific_type), 'rendering')\n",
    "                temp = os.listdir(path1)\n",
    "                \n",
    "                for file_name in temp:\n",
    "                    \n",
    "                    if file_name.endswith('.png'):\n",
    "                        path = os.path.join(path1, file_name)\n",
    "                        \n",
    "                        self.ids.append((image_type, specific_type, path))\n",
    "            \n",
    "        self.l = glob.glob('pix3d/*/*.png')\n",
    "        self.n = len(self.l)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (actual image and point cloud).\"\"\"\n",
    "        image_root = self.image_root\n",
    "        point_cloud_root = self.point_cloud_root\n",
    "        \n",
    "        image_type, specific_type, image_path = self.ids[index]\n",
    "        \n",
    "        # Load image and point cloud and return\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        real_image = Image.open(self.l[random.randint(0, self.n)]).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            real_image = self.transform(real_image)   \n",
    "        \n",
    "        if self.use_2048:\n",
    "            point_cloud_path = os.path.join(os.path.join(os.path.join(point_cloud_root, image_type), \n",
    "                                                     specific_type), 'pointcloud_2048.npy')\n",
    "        else:\n",
    "            point_cloud_path = os.path.join(os.path.join(os.path.join(point_cloud_root, image_type), \n",
    "                                                     specific_type), 'pointcloud_1024.npy')\n",
    "            \n",
    "        point_cloud = np.load(point_cloud_path)\n",
    "        \n",
    "        return image, point_cloud, real_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def get_loader(image_root, point_cloud_root, use_2048, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom X dataset.\"\"\"\n",
    "    \n",
    "    xdataset = XDataset(image_root, point_cloud_root, transform = transform, use_2048 = use_2048)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset=xdataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers)\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "image_root = \"./../../../datasets/cs253-wi20-public/ShapeNetRendering/\"\n",
    "point_cloud_root = \"./../../../datasets/cs253-wi20-public/ShapeNet_pointclouds/\"\n",
    "\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "num_workers = 8\n",
    "use_2048 = True\n",
    "img_size = 256\n",
    "transform = transforms.Compose([transforms.Resize(img_size,interpolation=2),transforms.CenterCrop(img_size),transforms.ToTensor()])\n",
    "data_loader = get_loader(image_root, point_cloud_root, use_2048, transform, batch_size, shuffle, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "f = _F_().cuda()\n",
    "d_domain = Destructor_domain().cuda()\n",
    "d_shape = Destructor_shape().cuda()\n",
    "optimizerf = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "optimizer_domain = torch.optim.Adam(d_domain.parameters(), lr=learning_rate)\n",
    "optimizer_shape = torch.optim.Adam(d_shape.parameters(), lr=learning_rate)\n",
    "\n",
    "enc = torch.load('encoder').cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_f = 0.0\n",
    "    loss_domain = 0.0\n",
    "    loss_shape = 0.0\n",
    "    \n",
    "    for (i_syn, pc_syn, i_real) in data_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            i_syn = i_syn.cuda()\n",
    "            i_real = i_real.cuda()\n",
    "            pc_syn = pc_syn.transpose(1,2).cuda()\n",
    "            \n",
    "        #real =1; syn =0\n",
    "        enc_syn = f(i_syn)\n",
    "        output = d_domain(enc_syn)\n",
    "        errD_syn = criterion(output, torch.full((batch_size,), 1, device=i_syn.device))\n",
    "        errG_syn = criterion(output, torch.full((batch_size,), 0, device=i_syn.device))\n",
    "        enc_real = f(i_real)\n",
    "        output = d_domain(enc_real)\n",
    "        errD_real = criterion(output, torch.full((batch_size,), 0, device=i_syn.device))\n",
    "        errG_real = criterion(output, torch.full((batch_size,), 1, device=i_syn.device))\n",
    "        errD = errD_syn + errD_real\n",
    "        errD.backward(retain_graph=True)\n",
    "        optimizer_domain.step()\n",
    "        errG = errG_syn + errG_real\n",
    "        errG.backward()\n",
    "        optimizerf.step()\n",
    "        \n",
    "        enc_pc = enc(pc_syn.float())\n",
    "        output = d_shape(enc_pc)\n",
    "        errD_pc = criterion(output, torch.full((batch_size,), 1, device=i_syn.device))\n",
    "        errG_pc = criterion(output, torch.full((batch_size,), 0, device=i_syn.device))\n",
    "        output = d_shape(enc_syn)\n",
    "        errD_syn1 = criterion(output, torch.full((batch_size,), 0, device=i_syn.device))\n",
    "        errG_syn1 = criterion(output, torch.full((batch_size,), 1, device=i_syn.device))\n",
    "        output = d_shape(enc_real)\n",
    "        errD_real1 = criterion(output, torch.full((batch_size,), 0, device=i_syn.device))\n",
    "        errG_real1 = criterion(output, torch.full((batch_size,), 1, device=i_syn.device))\n",
    "        lossD = errD_pc + errD_syn1 + errD_real1\n",
    "        lossG = errG_pc + errG_syn1 + errG_real1\n",
    "        lossD.backward(retain_graph=True)\n",
    "        optimizer_shape.step()\n",
    "        lossG.backward()\n",
    "        optimizerf.step()\n",
    "        \n",
    "        loss_f+=errG.data.detach() +lossG.data.detach() \n",
    "        loss_domain+=errD.data.detach() \n",
    "        loss_shape +=lossD.data.detach() \n",
    "        \n",
    "    print('epoch [{}/{}], F_loss:{:.4f}, Domain_loss:{:.4f}, Shape_loss:{:.4f}'.format(epoch + 1, num_epochs, loss_f/i_syn.shape[0], loss_shape/i_syn.shape[0], ))\n",
    "        \n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
