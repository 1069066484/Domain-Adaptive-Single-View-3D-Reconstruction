{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class XDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, image_root, point_cloud_root, transform=None, use_2048 = True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path: image directory.\n",
    "            \n",
    "        \"\"\"\n",
    "        self.image_root = image_root\n",
    "        self.point_cloud_root = point_cloud_root\n",
    "        self.transform = transform\n",
    "        self.use_2048 = use_2048\n",
    "        \n",
    "        \n",
    "        # Init ids\n",
    "        self.ids = []\n",
    "        \n",
    "        # Store ids\n",
    "        \n",
    "        image_types = os.listdir(image_root)\n",
    "        file_names = []\n",
    "        \n",
    "        for image_type in image_types:\n",
    "            if image_type.endswith('.tgz'):\n",
    "                    continue\n",
    "            \n",
    "            specific_types = os.listdir(os.path.join(image_root, image_type))\n",
    "            \n",
    "            for specific_type in specific_types:\n",
    "                if specific_type.endswith('.tgz'):\n",
    "                    continue\n",
    "                \n",
    "                path1 = os.path.join(os.path.join(os.path.join(image_root, image_type), specific_type), 'rendering')\n",
    "                temp = os.listdir(path1)\n",
    "                \n",
    "                for file_name in temp:\n",
    "                    \n",
    "                    if file_name.endswith('.png'):\n",
    "                        path = os.path.join(path1, file_name)\n",
    "                        \n",
    "                        self.ids.append((image_type, specific_type, path))\n",
    "            \n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (actual image and point cloud).\"\"\"\n",
    "        image_root = self.image_root\n",
    "        point_cloud_root = self.point_cloud_root\n",
    "        \n",
    "        image_type, specific_type, image_path = self.ids[index]\n",
    "        \n",
    "        # Load image and point cloud and return\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        image = np.asarray(image)\n",
    "            \n",
    "            \n",
    "        \n",
    "        if self.use_2048:\n",
    "            point_cloud_path = os.path.join(os.path.join(os.path.join(point_cloud_root, image_type), \n",
    "                                                     specific_type), 'pointcloud_2048.npy')\n",
    "        else:\n",
    "            point_cloud_path = os.path.join(os.path.join(os.path.join(point_cloud_root, image_type), \n",
    "                                                     specific_type), 'pointcloud_1024.npy')\n",
    "            \n",
    "        point_cloud = np.load(point_cloud_path)\n",
    "        \n",
    "        return image, point_cloud\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def get_loader(image_root, point_cloud_root, use_2048, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom X dataset.\"\"\"\n",
    "    \n",
    "    xdataset = XDataset(image_root, point_cloud_root, transform = transform, use_2048 = use_2048)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset=xdataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = 4\n",
    "class _F_(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 256 x 256\n",
    "            nn.Conv2d(3, int(ndf/2) , 4, stride=2, padding=1, bias=False), \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf/2) x 128 x 128\n",
    "            nn.Conv2d(int(ndf/2), ndf, 4, stride=2, padding=1, bias=False), \n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 64 x 64\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 32 x 32\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 16 x 16 \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 8 x 8\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*16) x 4 x 4\n",
    "            nn.Conv2d(ndf * 16, 1, 4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # state size. 1\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "    \n",
    "class Destructor_domain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128,1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        out = F.relu(self.bn1(self.fc1(input)))\n",
    "        out = F.relu(self.bn2(self.fc2(out)))\n",
    "        out = F.relu(self.bn3(self.fc3(out)))\n",
    "        return F.sigmoid(self.fc4(out))\n",
    "    \n",
    "    \n",
    "class Destructor_shape(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128,1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.view(input.size(0), -1)\n",
    "        out = F.relu(self.bn1(self.fc1(input)))\n",
    "        out = F.relu(self.bn2(self.fc2(out)))\n",
    "        out = F.relu(self.bn3(self.fc3(out)))\n",
    "        return F.sigmoid(self.fc4(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "f = _F_().cuda()\n",
    "d_domain = Destructor_domain().cuda()\n",
    "d_shape = Destructor_shape().cuda()\n",
    "optimizerf = torch.optim.Adam(f.parameters(), lr=learning_rate)\n",
    "optimizer_domain = torch.optim.Adam(d_domain.parameters(), lr=learning_rate)\n",
    "optimizer_shape = torch.optim.Adam(d_shape.parameters(), lr=learning_rate)\n",
    "\n",
    "enc = torch.load('encoder').cuda()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for (i_syn, i_real, pc_syn) in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            i_syn = i_syn.cuda()\n",
    "            i_real = i_real.cuda()\n",
    "            pc_syn = pc_syn.cuda()\n",
    "            \n",
    "        #real =1; syn =0\n",
    "        enc_syn = f(i_syn)\n",
    "        output = d_domain(enc_syn)\n",
    "        errD_syn = criterion(output, torch.full((b_size,), 1, device=device))\n",
    "        errG_syn = criterion(output, torch.full((b_size,), 0, device=device))\n",
    "        enc_real = f(i_real)\n",
    "        output = d_domain(enc_real)\n",
    "        errD_real = criterion(output, torch.full((b_size,), 0, device=device))\n",
    "        errG_real = criterion(output, torch.full((b_size,), 1, device=device))\n",
    "        errD = errD_syn + errD_real\n",
    "        errD.backward()\n",
    "        optimizer_domain.step()\n",
    "        errG = errG_syn + errG_real\n",
    "        errG.backward()\n",
    "        optimizerf.step()\n",
    "        \n",
    "        enc_pc = enc(pc_syn)\n",
    "        output = d_shape(enc_pc)\n",
    "        errD_pc = criterion(output, torch.full((b_size,), 1, device=device))\n",
    "        errG_pc = criterion(output, torch.full((b_size,), 0, device=device))\n",
    "        output = d_shape(enc_syn)\n",
    "        errD_syn1 = criterion(output, torch.full((b_size,), 0, device=device))\n",
    "        errG_syn1 = criterion(output, torch.full((b_size,), 1, device=device))\n",
    "        output = d_shape(enc_real)\n",
    "        errD_real1 = criterion(output, torch.full((b_size,), 0, device=device))\n",
    "        errG_real1 = criterion(output, torch.full((b_size,), 1, device=device))\n",
    "        lossD = errD_pc + errD_syn1 + errD_real1\n",
    "        lossG = errG_pc + errG_syn1 + errG_real1\n",
    "        lossD.backward()\n",
    "        optimizer_shape.step()\n",
    "        lossG.backward()\n",
    "        optimizerf.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
